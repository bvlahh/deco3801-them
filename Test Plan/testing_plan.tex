\documentclass[10pt]{article}

\pagestyle{plain}

\setlength{\textwidth}{5.9truein}
\setlength{\textheight}{8.7truein}
\setlength{\oddsidemargin}{2.0mm}
\setlength{\evensidemargin}{2.0mm}
\setlength{\topmargin}{-20.5truemm}
\setlength{\parindent}{0.0truemm}
\parskip=1mm

\title{\bf DECO3801 Test Plan Document}
\author{\normalsize THEM - Typed HTML5 Evaluation Machine \\ \normalsize Carl Hattenfels, Scott Heiner, Shen Yong Lau, Robert Meyer, Brendan Miller, David Uebergang}

\date{}

\begin{document}

\maketitle

\section*{Functional Test Plan}

\subsection*{Testing Strategy}

There are three major testable components of the Typed HTML5 Evaluation Machine, our web application: the front-end website, back-end parser and database. While it was easy to write Python test cases for the back-end parser, it was more difficult to test our front-end website and database with a suite of computer-run tests. Instead, we wrote up a series of scenarios that we would undertake to ensure that the web application was running correctly and as expected. Clearly, all of these scenario tests can be ``implemented" as they are merely actions performed by us. This means that a test fails when some functionality is not yet implemented, or when fixing one error creates another error.

\subsection*{Test Case Transcript}

% just put the excel stuff here

\subsection*{Implications of Functional Testing}

Our functional testing highlighted some issues with all aspects of our application.

%can talk about unimplemented tests here

\newpage

\section*{User Experience Goals}

We had a clear user experience in mind while developing this website. Through its ease of use and minimal effort on the part of the user, we have aimed to create a very surgical, ambient, passive experience. The tool should give users immediate insight into the issues with their HTML and websites. This is where the user's experience with our tool ends, for this session. The user now can go and fix their file externally, return to our program and almost instantly receive another assessment on their code.

- user shouldn't get invested in the system, nor get frustrated by the errors
- it is meant to be a program you just ``touch", that is, upload your file you want to check, and then go back and fix it, and then come back to this to validate again, in a cyclic process. priorities are on quick and easy use, which is why everything is instantly accessible and requires very few clicks to navigate.

\newpage

\section*{User Testing Plan}

- poor initial consideration due to highly targeted user base - DECO1400 students
- primarily focused on students who hadn't done deco1400 as these represent students ``new" to deco1400
- also focused on masters students who had done deco7140, both for their opinions of the tool and whether it would be relevant to the course
- attempted to get undergraduated students who had done deco1400 before, but poor communication and initial consideration (mentioned above) on our part lead to us getting very few in this user category

- user groups
  - undergraduate students who have already done DECO1400
  - undergraduate students who haven't done DECO1400 but have worked with computers
  - masters students who have already done DECO7140
  - masters students who have not already done DECO7140

- in general, users had no trouble navigating the system
- the result of the validation (i.e. highlighted tags) was not well understood by users

Summarise the results of your tests. 
For each scenario-based test: 
• tabulate your metric results against each task 
• describe or present your users’ feedback during/after the test 
Close with a general discussion that: 
• summarises the issues raised 
• identifies areas for improvement and design suggestions 
• outlines any redefinition of functional and user test plans for final 
prototype 
(We don’t care whether your prototype ‘passes’ the tests. What is 
important is that your prototype is sufficiently broad to validate your test 
plan for the final product.)

\end{document}